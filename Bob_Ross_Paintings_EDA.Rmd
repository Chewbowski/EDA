---
title: "Exploratory Data Analysis in R"
author: "Hugh.Welch@va.gov"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  html_document:
   code_folding: hide
   toc: TRUE
   toc_float: TRUE
   toc_depth: 1
editor_options:
 chunk_output_type: console
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      fig.width = 9,
                      fig.height = 6,
                      echo = TRUE)

```


# Introduction

**About EDA**  
Exploratory Data Analysis (EDA) is the process of conducting a first pass at 
a new or poorly understood set of data with the goal of exploring the shape, 
structure, and contents of the dataset.  
EDA can help Analysts and Data Scientists to understand the scope of the data 
and how much work will likely be required to clean and transform the dataset into 
a usable and useful format. The purpose of EDA is not necessarily to answer 
questions about the content of the dataset, but to find valuable questions to ask 
of it. This is accomplished through a number of techniques ranging from simple 
summary statistics and data visualization to relatively sophisticated modelling 
to understand the nature of patterns and relationships in the data.  
EDA can be accomplished via a number of means however language based approaches 
offer inherent repeatability and reproducibility, along with the option to 
document the potentially complex processes of data transformation and cleaning 
in-line with the code. Additionally, work done through scripting language is 
readily extensible for further development and can form the framework for rapid 
prototyping and eventually production.

<br />

**About the R Language**  
*R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS.* - [https://www.r-project.org/](https://www.r-project.org/)  
The R language was developed primarily to work with data and statistical analysis 
and as such is uniquely suited for EDA work. It is an open source language with 
a number of packages that extend it's functionality and has an incredibly 
collaborative and supportive user base.

<br />

**Relevant Operator Definitions**  
**<-** functions as the assignment operator in R and can be read as from right to left as  
[This_Variable_Name] <- (assign as) [This_Object]  
  
**%>%** is the pipe operator in the R Tidyverse and is used to string together operations. 
It can be read from left to right as  
[Do this operation] %>% (and then) [Do this other operation]  

***

<br />


# Exploratory Analysis


### Load Frequently Used Packages

```{r load_packages}

library(tidyverse)
library(plotly)

```

The tidyverse is a metapackage comprised of multiple packages designed to 
extend the functionality of the R language and to work well together.  
The plotly package creates client or server side interactive data visualizations.

***

<br />

### Import Data

```{r import_data}

bob_ross_df <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-21/bob_ross.csv')

```

This is a publicly available dataset featured in the weekly Tidy Tuesday R 
community github repository.

***

<br />

### What is the shape and structure of the dataset?

**Dataset shape**

```{r data_shape}

glimpse(bob_ross_df)

```

The dataset is comprised of **403 rows** across **27 columns**. Each record in 
the dataset appears to be a single episode with descriptive information.

***

<br />

**Dataset structure**

```{r data_structure}

library(skimr)

skim(bob_ross_df) %>% 
  arrange(desc(logical.mean))

```

Thankfully this dataset has no records with missing information and requires 
little to no cleaning and modification before analysis can begin. Note that the 
use of colors show by num_colors in the numeric variables appears to have a slightly 
left skewed distribution.

***

<br />

**Further EDA**

```{r dataexplorer_example, fig.cap='Fig 1, Color Use per Season Correlation Plot'}

library(DataExplorer)

bob_ross_df %>%
  group_by(season) %>%
  summarise(across(c(Black_Gesso:Alizarin_Crimson),
                   mean)) %>%
  ungroup() %>% 
  plot_correlation(title = 'Color Use per Season Correlation Plot',
                   theme_config = list(title = element_text(face = 'bold'),
                                       axis.text.x = element_text(angle = 90),
                                       legend.position = 'bottom'))

```

The DataExplorer package is an incredibly powerful tool for exploring datasets 
in depth with minimal coding. Here we see one function from the package for 
creating correlation plots. It looks like there are several possible color 
and season correlations worth exploring further.

***

<br />

### How many episodes did he record per season?

```{r episode_count}

episode_count_df <-
  bob_ross_df %>%
  count(season, name = 'episodes_per_season') %>%
  count(`episodes_per_season`, name  = 'number_of_seasons')

episode_count_df

```

Bob recorded **`r episode_count_df$number_of_seasons`** seasons comprised of **`r episode_count_df$episodes_per_season`** episodes each!


***

<br />

### Did the number of colors used change per season?

```{r colors_line_chart, fig.cap='Fig 2, Color Count per Episode Line Chart'}

library(htmlwidgets)

bob_ross_df %>%
  transmute(
    episode = paste0('S', season, ':Ep', episode),
    srt = row_number(),
    num_colors,
    painting_title,
    img_src
  ) %>%
  plot_ly(
    x = ~ fct_reorder(episode, srt),
    y = ~ num_colors,
    color = I('#1A237E'),
    type = 'scatter',
    mode = 'lines+markers',
    text = ~ paste0(
      'Episode: <b>',
      episode,
      '</b><br>',
      'Count of Colors: <b>',
      num_colors,
      '</b><br>',
      'Title: <b>',
      painting_title,
      '</b><br>',
      '<i>click point to open painting</i>'
    ),
    hoverinfo = 'text',
    customdata = ~ img_src
  ) %>%
  layout(
    title = list(text = '<b>Colors Used per Episode</b>\n<i> hover over points for more info</i>',
                 x = 0.05),
    xaxis = list(title = FALSE),
    yaxis = list(title = '<b>Color Count</b>')
  ) %>%
  onRender(
    "
function(el, x) {
  el.on('plotly_click', function(d) {
    var point = d.points[0];
    var url = point.data.customdata[point.pointIndex];
    window.open(url);
  });
}"
  )

```

We see an initial upswing in the number of colors used per episode that becomes 
relatively stable in later seasons. By putting additional information in the 
hover over tooltip, we ease the cognitive burden of the audience. Additionally, 
simple JavaScript functions can extend the visualization functionality with 
options such as the one demonstrated here for linking to the described painting. 

***

<br />

### Did the ratio of colors change over time?

**Density plot of individual color usage rates by season**

```{r density_ridge, fig.cap='Fig 3.1, Color Use Distribution Density Ridge Plot'}

library(ggridges)

bob_ross_df %>% 
  select(season,
         Black_Gesso:Alizarin_Crimson) %>% 
  pivot_longer(cols = !matches('season')) %>% 
  group_by(season,
           name) %>% 
  summarise(use_rate = mean(value)) %>% 
  ungroup() %>% 
  group_by(name) %>% 
  mutate(name_srt = mean(use_rate)) %>% 
  ungroup() %>% 
  ggplot(aes(x = use_rate,
             y = fct_reorder(name, name_srt))) +
  geom_density_ridges_gradient(aes(fill = after_stat(x)),
                               scale = 1.5,
                               color = '#FFFFFF') +
  scale_fill_gradient(low = '#C5CAE9',
                      high = '#1A237E',
                      name = 'Use Rate') +
  labs(title = 'Color Use Rate per Season Distribution',
       x = 'Use Rate per Season',
       y = NULL)

```

This is a density ridge plot with a gradient fill that shows the distribution of 
color usage rates per season across all colors. We can see that Titanium White 
was highly used, Indian Red infrequently used, and Phthalo Blue had a highly 
variable use per season.

***

```{r colors_stacked_barchart, fig.cap='Fig 3.2, Color Use Rate Stacked Barchart'}

colors_per_season_df <-
  bob_ross_df %>%
  group_by(season) %>%
  summarise(across(c(Black_Gesso:Alizarin_Crimson),
                   ~ mean(.))) %>%
  ungroup() %>%
  pivot_longer(cols = !contains('season')) %>%
  mutate(name = str_replace_all(name, '_', ' '))

colors_df <-
  bob_ross_df %>%
  select(colors,
         color_hex) %>%
  mutate(across(everything(),
                ~ str_extract(., "'.*'[,|\\]]"))) %>%
  mutate(across(everything(),
                ~ str_remove_all(., "\\\\r|\\\\n|'|\\]"))) %>%
  separate_longer_delim(cols = everything(),
                        delim = ',') %>%
  mutate(across(everything(),
                ~ str_squish(.))) %>%
  distinct() %>%
  rename(name = colors)

colors_per_season_df %>%
  left_join(colors_df,
            na_matches = 'never') %>%
  highlight_key(~ name) %>% 
  plot_ly(x = ~ season,
          y = ~ value) %>%
  add_bars(
    color = ~ I(color_hex),
    marker = list(line = list(width = 1,
                              color = 'black')),
    text = ~ paste0(
      'Season: <b>',
      season,
      '</b><br>',
      'Color: <b>',
      name,
      '</b><br>',
      'Use Rate: <b>',
      round(value, 2),
      '</b>'
    ),
    hoverinfo = 'text',
    textposition = 'none'
  ) %>%
  layout(barmode = 'stack',
         title = list(text = '<b>Color Usage Rate per Season</b>\n<i> hover over bars for more info, click bar to highlight</i>',
                      x = 0.05),
         xaxis = list(title = '<b>Season</b>'),
         yaxis = list(title = '<b>Color Usage Rate</b>'))

```

By using relatively simple regular expressions (regex) supported in R, we can 
extract the actual colors by their hexadecimal code and map them to their 
respective bars in this visualization.

***

<br />

### What were common title topics?

```{r common_topics, fig.cap='Fig 4, Top Painting Title Term Frequency Barchart'}

library(tidytext)

title_tokens_df <-
  bob_ross_df %>%
  select(season,
         painting_title) %>%
  unnest_tokens(input = painting_title,
                output = 'word',
                drop = FALSE)

title_tokens_df %>%
  anti_join(stop_words) %>%
  mutate(word = str_remove(word, 's$')) %>%
  count(word) %>%
  slice_max(order_by = n,
            n = 25) %>%
  plot_ly(x = ~ n,
          y = ~ fct_reorder(word, n)) %>%
  add_bars(
    color = I('#1A237E'),
    text = ~ paste0('Word: <b>', word, '</b><br>',
                    'Count: <b>', n, '</b>'),
    hoverinfo = 'text',
    textposition = 'none'
  ) %>%
  layout(
    title = list(text = '<b>Frequently Used Words in Painting Title</b>',
                 x = 0.05),
    xaxis = list(title = '<b>Word Count</b>'),
    yaxis = list(
      title = FALSE,
      tickprefix = '<b>',
      ticksuffix = '</b> '
    )
  )

```

The tidytext package greatly simplifies the steps required for text mining. 
Here we can easily unnest the painting titles into columns of their discrete 
words for further analysis.

***

<br />

### Did topics change by season?

```{r topic_change_trellis, fig.cap='Fig 5.1, Top Topic Weighted Log Odds by Season Trellis Chart'}

library(tidylo)

title_lo_top_df <-
  title_tokens_df %>%
  mutate(word = str_remove(word, 's$')) %>%
  count(season,
        word) %>%
  bind_log_odds(set = season,
                feature = word,
                n = n) %>%
  anti_join(stop_words) %>%
  slice_max(order_by = log_odds_weighted,
            n = 50)

title_lo_top_df %>%
  ggplot(aes(x = log_odds_weighted,
             y = fct_reorder(word, log_odds_weighted))) +
  geom_col(fill = '#1A237E') +
  facet_wrap(vars(season),
             scales = 'free') +
  theme(axis.title.y = element_blank())

```

The tidylo package is small but powerful in that it enables the calculation 
of weighted log odds with uninformative Dirichlet priors. This gives an improvement 
when compared to the tidytext package's Term Frequency vs Inverse Document 
Frequency (TF-IDF) when dealing with small term frequencies. It is worth noting 
that recent research indicates that the inclusion of stop words in the 
initial calculation or further modelling efforts generates more accurate results, 
however it is still good practice to remove them when communicating results to 
lay audiences to prevent misinterpretation.

***

```{r topic_heatmap, fig.cap='Fig 5.2, Top Topic Weighted Log Odds Heatmap'}

title_lo_top_df %>%
  group_by(season) %>%
  group_by(word) %>%
  mutate(word_srt = mean(log_odds_weighted)) %>%
  ungroup() %>%
  plot_ly(x = ~ season,
          y = ~ fct_reorder(word, word_srt)) %>%
  add_heatmap(
    z = ~ log_odds_weighted,
    xgap = 1,
    ygap = 1,
    text = ~ paste0(
      'Season: <b>',
      season,
      '</b><br>',
      'Title Word: <b>',
      word,
      '</b><br>',
      'Wt. Log Odds: <b>',
      round(log_odds_weighted, 2),
      '</b>'
    ),
    hoverinfo = 'text'
  ) %>%
  layout(title = list(text = '<b>Topic Heatmap</b>',
                      x = 0.05),
         yaxis = list(title = FALSE),
         xaxis = list(title = '<b>Season</b>'))

```

This heatmap is an alternate visual for showing important topics by season. The 
advantage here is the ability to follow a topic or season across an axis, 
however the trade off is a slight decrease in readability.

***

<br />

# Predictive Model

## Can we predict if a painting is a Winter scene from the range of colors used?



### First we prepare a dataframe for modelling


```{r model_prep}

library(tidymodels)
library(themis)

# prepare dataframe for modelling 
model_data_df <-
  bob_ross_df %>%
  select(painting_title,
         Black_Gesso:Alizarin_Crimson) %>%
  mutate(has_winter = if_else(str_detect(str_to_lower(painting_title),
                                           'winter'),
                                '1_winter',
                                '2_no_winter')) %>%
  select(-painting_title) %>%
  mutate(across(c(Black_Gesso:Alizarin_Crimson),
                ~ if_else(., 1, 0)))

glimpse(model_data_df)

```

Creating a separate dataframe of only the outcome we want to predict and the 
features we would like to use for prediction eases additional downstream transformations. 
Depending on the final modelling algorithm used, this can be an iterative step.

***

<br />

### Next we split the data into training and testing sets

```{r create_data_splits}

# create training and testing data splits

set.seed(42)

raw_split <- model_data_df %>%
  initial_split(strata = has_winter)

raw_train <- training(raw_split)

raw_test <- testing(raw_split)

raw_split

```

Here we see the ~ 70/30 training/testing split of our data. We want to train and
assess the potential model while keeping the testing portion as a holdout to be 
used only once for final model evaluation.

***

<br />

### Now we create additional bootstrap folds for training and cross validation

```{r create_data_folds}

# create bootstrap data folds for training and cross validation assessment

set.seed(123)

raw_folds <- vfold_cv(raw_train,
                      strata = has_winter)

raw_folds

```

Because this dataset is so small, we will create additional folds of the dataset. 
This is done through bootstrap resampling and will result in number of folds 
used to assess and modify the model during training through cross-fold 
validation.

***

<br />

### Then we create a model workflow comprised of a step recipe and model specs

```{r create_recipe_and_specs}

# create model step recipe
raw_rec <-
  recipe(has_winter ~ .,
                  data = raw_train) %>%
  step_upsample(has_winter) %>%
  step_dummy(all_nominal_predictors())

# set model algorithm and specs
xgb_spec <-
  boost_tree(
    trees = tune(),
    mtry = tune(),
    min_n = tune(),
    learn_rate = 0.1
  ) %>%
  set_engine('xgboost') %>%
  set_mode('classification')

# create model workflow
xgb_wf <- workflow(raw_rec, xgb_spec)

xgb_wf

```

The tidymodels metapackage makes it easy to string together the common modelling 
steps into recipes and workflows that are easy for humans to read and comprehend. 
Here we have a dataset well suited to decision tree models and will be using 
boosted decision trees with the popular XGBoost algorithm. This algorithm 
has a large number of hyperparameters we could tune to change model performance, 
however here we will use some of the built-in package functionality to tune 
the major hyperparameters for us on the already built training folds.

***

<br />

### Now we tune the model hyperparameters

**Here we can see the results of the model tuning**

```{r tune_model, fig.cap='Fig 6.1, ANOVA Race Tuning'}

library(finetune)

# turn on parallel processing
doParallel::registerDoParallel()

# Tune the model hyperparameters
set.seed(628)

xgb_raw_rs <-
  tune_race_anova(
    xgb_wf,
    raw_folds,
    grid = 20,
    control = control_race(verbose_elim = TRUE)
  )

plot_race(xgb_raw_rs)

show_best(xgb_raw_rs)

```

The Race ANOVA tuning function is nice in that as hyperparameter combinations 
are shown to be less performant they are dropped from further modelling reducing 
total time and resources. Here we can see the separate trials and the best 
performing variants. 

***

<br />

### Finally we fit the model to our testing holdout


```{r fit_predictions, fig.cap='Fig 6.2, Final Model Fit AUC Chart'}

xgb_last <-
  xgb_wf %>%
  finalize_workflow(select_best(xgb_raw_rs, 'roc_auc')) %>%
  last_fit(raw_split)

xgb_fit <- extract_fit_parsnip(xgb_last)

final_preds <-
  xgb_last %>%
  collect_predictions()

final_preds %>%
  roc_curve(has_winter, .pred_1_winter) %>%
  plot_ly(
    x = ~ 1 - specificity,
    y = ~ sensitivity,
    showlegend = FALSE
  ) %>%
  add_lines(
    color = I('#1A237E'),
    line = list(width = 4),
    text = ~ paste0(
      'Specificity: <b>',
      round(specificity, 3),
      '</b><br>',
      'Sensitivity: <b>',
      round(sensitivity, 3),
      '</b>'
    ),
    hoverinfo = 'text'
  ) %>%
  add_segments(
    x = 0,
    xend = 1,
    y = 0,
    yend = 1,
    line = list(dash = 'dash'),
    color = I('#BDBDBD')
  ) %>%
  layout(xaxis = list(scaleanchor = 'y',
                      scaleratio = 1),
         title = list(text = '<b>ROC curve</b>',
                      x = 0.05))

```


```{r fit_metrics}

collect_metrics(xgb_last)

```


```{r fit_confusion_matrix}

final_preds %>%
  conf_mat(has_winter, .pred_class)

```

We can see that our model seems to perform well, however we would potentially 
need to modify further depending on how the model will be deployed. Decision 
thresholds could be customized to decrease the number of false positives at the 
cost of increasing false negatives should absolute certainty be needed in 
prediction of a Winter painting.  
Again, these changes should be made using training data during the assessment 
stage and prior to this final fit but were not shown here for the sake of 
demonstration length.

***

<br />

### Lastly we explain the model

```{r explain_model, fig.cap='Fig 6.3, Final Model Fit SHAP Beeswarm Summary Plot'}

library(SHAPforxgboost)

FTR_Shap <-
  shap.prep(
    xgb_model = extract_fit_engine(xgb_fit),
    X_train = bake(
      prep(raw_rec),
      has_role('predictor'),
      new_data = NULL,
      composition = 'matrix'
    )
  )

shap.plot.summary(FTR_Shap) +
  labs(title = 'Color Use Impact on Likelihood of Winter Painting') +
  theme(title = element_text(face = 'bold'),
        axis.text.y = element_text(face = 'bold'))

```

Decisions trees are highly useful algorithms when model explainability is a must. 
SHAP values have recently been making a comeback as a way to explain both 
variable importance and impact on model outputs, but also variable interactions. 
The SHAPforxgboost package gives access to a number of data visualizations 
for SHAP values.

***

<br />


# Summary

**EDA Results**  
Over the course of **31 seasons** comprised of **13 episodes each**, Bob Ross painted a 
total of **403 paintings**!  
His subjects ranged from **forest cabins** to **winter landscape** scenes, and he was 
particularly fond of painting **mountains**. The topics he chose to paint were 
broad, but occasionally followed predictable patterns with several topics presenting 
across multiple seasons. **Season six** however seemed to be a turning point as the 
number of topics he painted varied wildly.  
His choice of colors ramped up over the first few seasons, but then was relatively 
steady through the full production run. His use of color seems to be associated 
with his choice of topic, with an initial model predicting **Winter** paintings 
showing promising early results.

<br />

**Recommendations**  
Further analysis is warranted regarding the predictability of all common topics 
given the color usage. Decision thresholds for predictions may need be to be 
customized as this is a small dataset with wide ranging topics.  
A literature review could also be helpful in identifying additional data regarding 
his changing choice of topics and slightly evolving use of colors and styles.  


<br />

***

*End of Document*














